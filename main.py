import os
import time
import requests
import feedparser
import trafilatura
from openai import OpenAI

# =========================
# 1) ã‚ãªãŸå‘ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ–½å·¥ç®¡ç†ãƒ»è¨­å‚™ç®¡ç†ãƒ»å»ºè¨­Ã—äººæï¼‰
# =========================
KEYWORDS = [
    # æ–½å·¥ç®¡ç†ãƒ»è¨­å‚™ç®¡ç†
    "æ–½å·¥ç®¡ç†", "è¨­å‚™ç®¡ç†", "ãƒ“ãƒ«ãƒ¡ãƒ³", "æ–½è¨­ç®¡ç†", "FM", "ç¾å ´ç›£ç£",
    "é›»æ°—å·¥äº‹æ–½å·¥ç®¡ç†", "ç®¡å·¥äº‹æ–½å·¥ç®¡ç†", "å»ºç¯‰æ–½å·¥ç®¡ç†", "åœŸæœ¨æ–½å·¥ç®¡ç†",
    "é›»æ°—ä¸»ä»»æŠ€è¡“è€…", "ç›£ç†æŠ€è¡“è€…", "ä¸»ä»»æŠ€è¡“è€…", "æŠ€è¡“å£«",
    "1ç´šæ–½å·¥ç®¡ç†æŠ€å£«", "2ç´šæ–½å·¥ç®¡ç†æŠ€å£«", "å»ºç¯‰å£«",

    # äººæãƒ»è»¢è·å¸‚å ´ï¼ˆã‚ãªãŸã®ä»•äº‹ã«ç›´çµï¼‰
    "äººæ‰‹ä¸è¶³", "æ¡ç”¨", "è»¢è·", "è³ƒä¸Šã’", "åˆä»»çµ¦", "å¹´å", "æ®‹æ¥­", "2024å¹´å•é¡Œ",
    "åƒãæ–¹æ”¹é©", "æ™‚é–“å¤–åŠ´åƒ", "é€±ä¼‘2æ—¥", "36å”å®š",
    "æ´¾é£", "å¤–æ³¨", "å”åŠ›ä¼šç¤¾", "ä¸‹è«‹æ³•",

    # å»ºè¨­DX
    "BIM", "CIM", "i-Construction", "DX", "ç¾å ´DX", "é éš”è‡¨å ´", "é›»å­é»’æ¿",
    "ç©ç®—", "åŸä¾¡ç®¡ç†", "å·¥ç¨‹ç®¡ç†",

    # è¨­å‚™ãƒ»çœã‚¨ãƒï¼ˆè¨­å‚™ç®¡ç†ã«å¼·ã„æ­¦å™¨ï¼‰
    "çœã‚¨ãƒ", "ZEB", "ZEH", "è„±ç‚­ç´ ", "ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "BEMS",
    "ç©ºèª¿", "HVAC", "å†·å‡æ©Ÿ", "ãƒœã‚¤ãƒ©ãƒ¼", "æ¶ˆé˜²è¨­å‚™", "é˜²ç½", "ç‚¹æ¤œ", "æ³•æ”¹æ­£",

    # ç™ºæ³¨å´ãƒ»ç”¨é€”ï¼ˆè¨­å‚™ç®¡ç†ã‚­ãƒ£ãƒªã‚¢ã«åˆºã•ã‚‹ï¼‰
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼", "å·¥å ´", "ãƒ—ãƒ©ãƒ³ãƒˆ", "ç‰©æµå€‰åº«", "ç—…é™¢", "å•†æ¥­æ–½è¨­",
    "ä¸å‹•ç”£ç®¡ç†", "PM", "BM", "AM", "REIT"
]

# =========================
# 2) èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€åˆã¯ã“ã®ã¾ã¾ã§OKï¼‰
# =========================
MAX_ARTICLES_TO_SEND = 6          # 1å›ã«é€ã‚‹æœ€å¤§è¨˜äº‹æ•°ï¼ˆå¢—ã‚„ã™ã¨é•·æ–‡ã«ãªã‚Šã¾ã™ï¼‰
MAX_CHARS_FOR_SUMMARY = 6000      # AIã«æ¸¡ã™æœ¬æ–‡ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆã‚³ã‚¹ãƒˆã¨å®‰å®šæ€§ã®ãŸã‚ï¼‰
REQUEST_TIMEOUT = 20

# ã¾ãšã¯RSSãŒæ¯”è¼ƒçš„å–ã‚Šã‚„ã™ã„2ã¤ã‹ã‚‰é–‹å§‹ï¼ˆå®‰å®šç¨¼åƒã‚’å„ªå…ˆï¼‰
# â€»ã‚ã¨ã§5ã‚µã‚¤ãƒˆã«å¢—ã‚„ã—ã¾ã™
RSS_SOURCES = [
    ("ITmedia NEWS", "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"),
    ("æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³", "https://toyokeizai.net/list/feed/header"),
]

# =========================
# 3) Secretsï¼ˆGitHubã«ç™»éŒ²ã—ãŸã‚‚ã®ã‚’èª­ã‚€ï¼‰
# =========================
LINE_TOKEN = os.getenv("LINE_CHANNEL_TOKEN")
LINE_TO = os.getenv("LINE_TO_USER_ID")
OPENAI_KEY = os.getenv("OPENAI_API_KEY")

if not (LINE_TOKEN and LINE_TO and OPENAI_KEY):
    raise RuntimeError("Secretsä¸è¶³: LINE_CHANNEL_TOKEN / LINE_TO_USER_ID / OPENAI_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

client = OpenAI(api_key=OPENAI_KEY)

def keyword_hit(text: str) -> bool:
    t = text or ""
    return any(k in t for k in KEYWORDS)

def fetch_article_text(url: str) -> str:
    """
    ã§ãã‚‹ã ã‘æœ¬æ–‡ã‚’å–ã‚‹ã€‚
    å–ã‚Œãªã„ï¼ˆä¼šå“¡åˆ¶ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒ»æ§‹é€ çš„ã«é›£ã—ã„ï¼‰å ´åˆã¯ç©ºæ–‡å­—ã€‚
    ãã®å ´åˆã¯RSSã®æ¦‚è¦ï¼ˆsnippetï¼‰ã‚’è¦ç´„ã—ã¾ã™ã€‚
    """
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return ""
        extracted = trafilatura.extract(downloaded)
        return extracted or ""
    except Exception:
        return ""

def summarize(title: str, material: str, url: str) -> str:
    """
    AIè¦ç´„ã€‚å¿…ãšã€Œçµè«–/è¦ç‚¹/ãƒªãƒ³ã‚¯ã€å½¢å¼ã§è¿”ã™ã€‚
    """
    text = (material or "")[:MAX_CHARS_FOR_SUMMARY]
    prompt = f"""
ã‚ãªãŸã¯å»ºè¨­æ¥­ç•Œï¼ˆæ–½å·¥ç®¡ç†ãƒ»è¨­å‚™ç®¡ç†ï¼‰ã«ç‰¹åŒ–ã—ãŸã‚­ãƒ£ãƒªã‚¢ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼TLã®ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„è€…ã§ã™ã€‚
ä»¥ä¸‹ã®è¨˜äº‹ã‚’ã€æ—¥æœ¬èªã§ã€Œå®Ÿå‹™ã«å½¹ç«‹ã¤è¦–ç‚¹ã€ã§çŸ­ãã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘
{title}

ã€æœ¬æ–‡ï¼ˆã¾ãŸã¯æ¦‚è¦ï¼‰ã€‘
{text}

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšã“ã®3è¡Œæ§‹æˆï¼ˆä½™è¨ˆãªå‰ç½®ãä¸è¦ï¼‰
çµè«–ï¼šã€œï¼ˆ1è¡Œï¼‰
è¦ç‚¹ï¼šãƒ»ã€œ ãƒ»ã€œ ãƒ»ã€œï¼ˆæœ€å¤§3ã¤ï¼‰
ãƒªãƒ³ã‚¯ï¼š{url}
""".strip()

    resp = client.responses.create(
        model="gpt-4o-mini",
        input=prompt,
    )
    return resp.output_text.strip()

def push_line(message: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_TOKEN}",
    }
    body = {
        "to": LINE_TO,
        "messages": [{"type": "text", "text": message}],
    }
    r = requests.post(url, json=body, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()

def main():
    picked = []

    # 1) RSSå–å¾— â†’ ã‚¿ã‚¤ãƒˆãƒ«/æ¦‚è¦ã§å…ˆã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®šï¼ˆé«˜é€Ÿã§å®‰å®šï¼‰
    for source_name, rss_url in RSS_SOURCES:
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:30]:
            title = entry.get("title", "")
            link = entry.get("link", "")
            snippet = entry.get("summary", "") or entry.get("description", "") or ""
            if not link:
                continue
            if keyword_hit(title + " " + snippet):
                picked.append((source_name, title, link, snippet))

    if not picked:
        push_line("âœ… ä»Šæ—¥ã®è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãªã—ï¼‰")
        return

    # 2) ä¸Šé™ã¾ã§ã«çµã‚‹
    picked = picked[:MAX_ARTICLES_TO_SEND]

    # 3) æœ¬æ–‡å–å¾—â†’è¦ç´„â†’ã¾ã¨ã‚
    lines = ["ğŸ“° ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ï¼‰"]
    for source_name, title, link, snippet in picked:
        body = fetch_article_text(link)
        material = body if body.strip() else snippet  # æœ¬æ–‡ãŒå–ã‚Œãªã‘ã‚Œã°æ¦‚è¦ã§è¦ç´„
        summary = summarize(title, material, link)
        lines.append(f"\n\n{summary}")
        time.sleep(1)  # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã‚’å°‘ã—ã ã‘ã‚†ã£ãã‚Šã«

    # 4) LINEé€ä¿¡
    push_line("\n".join(lines))

if __name__ == "__main__":
    main()
