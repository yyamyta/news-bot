import os
import time
import requests
import feedparser
from urllib.parse import quote_plus
from datetime import datetime, timezone

# =========================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­è¨ˆï¼ˆã‚ãªãŸå‘ã‘ï¼‰
# =========================
ROLE_TERMS = [
    "æ–½å·¥ç®¡ç†", "ç¾å ´ç›£ç£", "è¨­å‚™ç®¡ç†", "æ–½è¨­ç®¡ç†", "ãƒ“ãƒ«ãƒ¡ãƒ³", "è¨­å‚™ä¿å…¨",
    "ã‚µãƒ–ã‚³ãƒ³", "ã‚¼ãƒã‚³ãƒ³", "ç®¡å·¥äº‹", "é›»æ°—å·¥äº‹", "ç©ºèª¿", "è¡›ç”Ÿ"
]

CANDIDATE_TERMS = [
    "è»¢è·", "æ±‚äºº", "æ¡ç”¨", "ä¸­é€”", "äººæç´¹ä»‹", "ã‚­ãƒ£ãƒªã‚¢",
    "å¹´å", "çµ¦ä¸", "è³ƒä¸Šã’", "æ®‹æ¥­", "ä¼‘æ—¥", "é€±ä¼‘2æ—¥", "æœ‰çµ¦",
    "åƒãæ–¹æ”¹é©", "åŠ´åƒæ™‚é–“", "æœªçµŒé¨“", "çµŒé¨“è€…", "è³‡æ ¼"
]

INDUSTRY_TERMS = [
    "2024å¹´å•é¡Œ", "åƒãæ–¹æ”¹é©", "æ™‚é–“å¤–åŠ´åƒ", "æ³•æ”¹æ­£", "å»ºè¨­æ¥­æ³•", "ä¸‹è«‹",
    "äººæ‰‹ä¸è¶³", "é«˜é½¢åŒ–", "ä¾¡æ ¼è»¢å«", "è³‡æé«˜é¨°", "å…¬å…±å·¥äº‹", "å»ºè¨­æŠ•è³‡",
    "BIM", "CIM", "DX", "é éš”è‡¨å ´",
    "çœã‚¨ãƒ", "ZEB", "è„±ç‚­ç´ "
]

MAX_ARTICLES = 12
REQUEST_TIMEOUT = 20

# =========================
# å–å¾—å…ƒï¼ˆæŒ‡å®š5ã‚µã‚¤ãƒˆï¼‰
# - RSSãŒå®‰å®šãªã‚‚ã®ã¯ç›´RSS
# - ãã‚Œä»¥å¤–ã¯ Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã§ site: ã—ã°ã‚Š
# =========================
def google_news_rss(query: str) -> str:
    return f"https://news.google.com/rss/search?q={quote_plus(query)}&hl=ja&gl=JP&ceid=JP:ja"

# ã€Œã‚µã‚¤ãƒˆç¸›ã‚Š + æ–½å·¥ç®¡ç†/è¨­å‚™ç®¡ç†ã£ã½ã„èªã€ãã‚‰ã„ã«ã—ã¦ã€å–ã‚Šé€ƒã—ã‚’æ¸›ã‚‰ã™
SITE_FEEDS = [
    ("æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒ†ãƒƒã‚¯", [
        google_news_rss("site:xtech.nikkei.com æ–½å·¥ç®¡ç†"),
        google_news_rss("site:xtech.nikkei.com è¨­å‚™ç®¡ç†"),
        google_news_rss("site:xtech.nikkei.com å»ºè¨­ äººæ‰‹ä¸è¶³"),
    ]),
    ("ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³", [
        google_news_rss("site:diamond.jp æ–½å·¥ç®¡ç†"),
        google_news_rss("site:diamond.jp å»ºè¨­ æ¡ç”¨"),
    ]),
    ("NewsPicks", [
        google_news_rss("site:newspicks.com å»ºè¨­ æ¡ç”¨"),
        google_news_rss("site:newspicks.com æ–½å·¥ç®¡ç†"),
    ]),
]

DIRECT_FEEDS = [
    ("ITmedia NEWS", ["https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"]),
    ("æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³", ["https://toyokeizai.net/list/feed/header"]),
]

SOURCES = DIRECT_FEEDS + SITE_FEEDS

# =========================
# LINEé€ä¿¡
# =========================
LINE_TOKEN = os.getenv("LINE_CHANNEL_TOKEN")
LINE_TO = os.getenv("LINE_TO_USER_ID")

if not (LINE_TOKEN and LINE_TO):
    raise RuntimeError("Secretsä¸è¶³: LINE_CHANNEL_TOKEN / LINE_TO_USER_ID ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def push_line(text: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LINE_TOKEN}"}
    body = {"to": LINE_TO, "messages": [{"type": "text", "text": text}]}
    r = requests.post(url, json=body, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def norm(s: str) -> str:
    s = (s or "").replace("\n", " ").replace("\r", " ").strip()
    while "  " in s:
        s = s.replace("  ", " ")
    return s

def count_hit(words, text: str) -> int:
    return sum(1 for w in words if w in text)

def get_published_dt(entry) -> datetime:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return datetime(1970, 1, 1, tzinfo=timezone.utc)
    return datetime(*t[:6], tzinfo=timezone.utc)

def split_for_line(message: str, limit: int = 4500):
    chunks, buf = [], ""
    for line in message.split("\n"):
        while len(line) > limit:
            head, line = line[:limit], line[limit:]
            if buf:
                chunks.append(buf); buf = ""
            chunks.append(head)
        if len(buf) + len(line) + 1 > limit:
            chunks.append(buf); buf = line
        else:
            buf = (buf + "\n" + line) if buf else line
    if buf:
        chunks.append(buf)
    return chunks

# =========================
# ãƒ¡ã‚¤ãƒ³ï¼ˆ2ã‚«ãƒ†ã‚´ãƒªï¼‰
# =========================
def main():
    items = []
    seen = set()

    for source_name, feed_urls in SOURCES:
        for feed_url in feed_urls:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:40]:
                title = norm(entry.get("title", ""))
                link = entry.get("link", "")
                summary = norm(entry.get("summary", "") or entry.get("description", ""))

                if not link or link in seen:
                    continue

                text = f"{title} {summary}"

                role_score = count_hit(ROLE_TERMS, text)
                cand_score = count_hit(CANDIDATE_TERMS, text)
                ind_score = count_hit(INDUSTRY_TERMS, text)

                # å»ºè¨­ãƒ»è¨­å‚™ã®æ–‡è„ˆãŒè–„ã„ã‚‚ã®ã¯è½ã¨ã™ï¼ˆãƒã‚¤ã‚ºå¯¾ç­–ï¼‰
                if role_score == 0 and ind_score == 0:
                    continue

                published = get_published_dt(entry)

                items.append({
                    "source": source_name,
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "published": published,
                    "role_score": role_score,
                    "cand_score": cand_score,
                    "ind_score": ind_score,
                })
                seen.add(link)

            time.sleep(0.2)

    if not items:
        push_line("âœ… ä»Šæ—¥ã®è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå»ºè¨­/è¨­å‚™æ–‡è„ˆã®ãƒ’ãƒƒãƒˆãªã—ï¼‰")
        return

    # A) æ±‚è·è€…å‘ã‘ï¼šè»¢è·ãƒ»å¾…é‡ç³»ã‚’å¼·ãè©•ä¾¡
    cand_items = sorted(
        items,
        key=lambda x: (x["cand_score"]*100 + x["role_score"]*20 + x["ind_score"]*5, x["published"]),
        reverse=True
    )

    # B) æ¥­ç•Œç†è§£ï¼šåˆ¶åº¦ãƒ»å¸‚å ´ãƒ»DXç­‰ã‚’å¼·ãè©•ä¾¡
    ind_items = sorted(
        items,
        key=lambda x: (x["ind_score"]*100 + x["role_score"]*20 + x["cand_score"]*5, x["published"]),
        reverse=True
    )

    cand_top = [x for x in cand_items if x["cand_score"] > 0][:6]
    ind_top  = [x for x in ind_items if x["ind_score"] > 0][:6]

    # ã©ã£ã¡ã‚‚ç©ºã«ãªã‚Šã†ã‚‹ã®ã§ä¿é™ºï¼ˆå½¹å‰²èªã§æ‹¾ãˆãŸã‚‚ã®ã‚’æœ€ä½é™é€ã‚‹ï¼‰
    if not cand_top and not ind_top:
        fallback = sorted(items, key=lambda x: (x["role_score"], x["published"]), reverse=True)[:6]
        msg = "ğŸ“° ä»Šæ—¥ã®å»ºè¨­/è¨­å‚™ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå‚è€ƒï¼‰\n"
        for it in fallback:
            summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
            msg += f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}\n"
        for chunk in split_for_line(msg):
            push_line(chunk); time.sleep(0.5)
        return

    lines = ["ğŸ§‘â€ğŸ’¼ æ±‚è·è€…å‘ã‘ï¼ˆè»¢è·ãƒ»å¾…é‡ãƒ»åƒãæ–¹ï¼‰"]
    for it in cand_top:
        summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
        lines.append(f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}")

    lines.append("\nğŸ—ï¸ æ¥­ç•Œç†è§£ï¼ˆåˆ¶åº¦ãƒ»å¸‚å ´ãƒ»DXãƒ»çœã‚¨ãƒï¼‰")
    for it in ind_top:
        summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
        lines.append(f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}")

    msg = "\n".join(lines)[:18000]  # å¿µã®ãŸã‚æš´èµ°é˜²æ­¢
    for chunk in split_for_line(msg):
        push_line(chunk)
        time.sleep(0.5)

if __name__ == "__main__":
    main()
