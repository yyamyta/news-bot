import os
import time
import requests
import feedparser
from urllib.parse import quote_plus
from datetime import datetime, timezone

# =========================
# 1) ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ±‚è·è€…å‘ã‘ã«çµã‚Šè¾¼ã¿ï¼‰
#    (è»¢è·ç³») AND (æ–½å·¥ç®¡ç†/è¨­å‚™ç®¡ç†ç³») ã‚’å¿…é ˆã«ã™ã‚‹
# =========================
MUST_CAREER = [
    "è»¢è·", "æ±‚äºº", "æ¡ç”¨", "ä¸­é€”", "äººæç´¹ä»‹", "ã‚­ãƒ£ãƒªã‚¢"
]

MUST_ROLE = [
    "æ–½å·¥ç®¡ç†", "ç¾å ´ç›£ç£", "è¨­å‚™ç®¡ç†", "æ–½è¨­ç®¡ç†", "ãƒ“ãƒ«ãƒ¡ãƒ³",
    "è¨­å‚™ä¿å…¨", "FM", "ãƒ•ã‚¡ã‚·ãƒªãƒ†ã‚£"
]

# ã‚ã‚‹ã¨â€œæ±‚è·è€…å‘ã‘ä¾¡å€¤â€ãŒé«˜ã„ã®ã§ä¸Šä½ã«ä¸¦ã¹ã‚‹ï¼ˆå¿…é ˆã§ã¯ãªã„ï¼‰
PRIORITY = [
    "å¹´å", "çµ¦ä¸", "è³ƒä¸Šã’",
    "æ®‹æ¥­", "ä¼‘æ—¥", "é€±ä¼‘2æ—¥", "æœ‰çµ¦",
    "åƒãæ–¹æ”¹é©", "åŠ´åƒæ™‚é–“",
    "æœªçµŒé¨“", "çµŒé¨“è€…", "è³‡æ ¼",
]

MAX_ARTICLES_TO_SEND = 10
REQUEST_TIMEOUT = 20

# =========================
# 2) å–å¾—å…ƒï¼ˆæŒ‡å®š5ã‚µã‚¤ãƒˆï¼‰
#    - RSSãŒã‚ã‚‹ã‚‚ã®ã¯ç›´RSS
#    - RSSãŒå®‰å®šã—ãªã„/æä¾›ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã‚‚ã®ã¯Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã§ã‚µã‚¤ãƒˆç¸›ã‚Š
# =========================
def google_news_rss_url(query: str) -> str:
    # æ—¥æœ¬å‘ã‘è¨­å®šï¼ˆhl/gl/ceidï¼‰
    return f"https://news.google.com/rss/search?q={quote_plus(query)}&hl=ja&gl=JP&ceid=JP:ja"

SOURCES = [
    # ç›´RSSï¼ˆå®‰å®šï¼‰
    ("ITmedia NEWS", "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"),
    ("æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³", "https://toyokeizai.net/list/feed/header"),

    # Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSï¼ˆã‚µã‚¤ãƒˆç¸›ã‚Šï¼‰
    ("æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒ†ãƒƒã‚¯", google_news_rss_url("site:xtech.nikkei.com")),
    ("ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³", google_news_rss_url("site:diamond.jp")),
    ("NewsPicks", google_news_rss_url("site:newspicks.com")),
]

# =========================
# 3) LINEé€ä¿¡ï¼ˆæ—¢å­˜ã®Secretsã‚’ä½¿ã†ï¼‰
# =========================
LINE_TOKEN = os.getenv("LINE_CHANNEL_TOKEN")
LINE_TO = os.getenv("LINE_TO_USER_ID")

if not (LINE_TOKEN and LINE_TO):
    raise RuntimeError("Secretsä¸è¶³: LINE_CHANNEL_TOKEN / LINE_TO_USER_ID ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def push_line(text: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_TOKEN}",
    }
    body = {"to": LINE_TO, "messages": [{"type": "text", "text": text}]}
    r = requests.post(url, json=body, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()

# =========================
# 4) ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def norm(s: str) -> str:
    s = (s or "").replace("\n", " ").replace("\r", " ").strip()
    while "  " in s:
        s = s.replace("  ", " ")
    return s

def any_hit(words, text: str) -> bool:
    return any(w in text for w in words)

def count_hit(words, text: str) -> int:
    return sum(1 for w in words if w in text)

def get_published_dt(entry) -> datetime:
    # feedparserã¯ published_parsed / updated_parsed ãŒã‚ã‚Œã°ä½¿ãˆã‚‹
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        # æ—¥ä»˜ä¸æ˜ã¯ã‹ãªã‚Šå¤ã„æ‰±ã„ã«ã™ã‚‹
        return datetime(1970, 1, 1, tzinfo=timezone.utc)
    return datetime(*t[:6], tzinfo=timezone.utc)

def split_for_line(message: str, limit: int = 4500):
    # LINEã®æ–‡å­—æ•°åˆ¶é™å›é¿ç”¨ï¼ˆå®‰å…¨å´ã§4500ï¼‰
    chunks = []
    buf = ""
    for line in message.split("\n"):
        # 1è¡ŒãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚‹
        while len(line) > limit:
            head, line = line[:limit], line[limit:]
            if buf:
                chunks.append(buf)
                buf = ""
            chunks.append(head)
        if len(buf) + len(line) + 1 > limit:
            chunks.append(buf)
            buf = line
        else:
            buf = (buf + "\n" + line) if buf else line
    if buf:
        chunks.append(buf)
    return chunks

# =========================
# 5) ãƒ¡ã‚¤ãƒ³
# =========================
def main():
    items = []
    seen_urls = set()

    for source_name, feed_url in SOURCES:
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:50]:
            title = norm(entry.get("title", ""))
            link = entry.get("link", "")
            summary = norm(entry.get("summary", "") or entry.get("description", ""))

            if not link or link in seen_urls:
                continue

            text = f"{title} {summary}"

            # (è»¢è·ç³») AND (æ–½å·¥ç®¡ç†/è¨­å‚™ç®¡ç†ç³») ã‚’å¿…é ˆ
            if not any_hit(MUST_CAREER, text):
                continue
            if not any_hit(MUST_ROLE, text):
                continue

            # ã‚¹ã‚³ã‚¢ï¼šæ±‚è·è€…å‘ã‘ãƒ†ãƒ¼ãƒèªãŒå¤šã„ã»ã©ä¸Šã«
            pr = count_hit(PRIORITY, text)
            role = count_hit(MUST_ROLE, text)
            career = count_hit(MUST_CAREER, text)
            score = pr * 100 + role * 10 + career

            published = get_published_dt(entry)

            items.append({
                "source": source_name,
                "title": title,
                "link": link,
                "summary": summary,
                "score": score,
                "published": published,
            })
            seen_urls.add(link)

        time.sleep(0.3)

    if not items:
        push_line("âœ… ä»Šæ—¥ã®è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆè»¢è·ç³» AND æ–½å·¥ç®¡ç†/è¨­å‚™ç®¡ç†ç³»ï¼‰")
        return

    # ã‚¹ã‚³ã‚¢å„ªå…ˆ â†’ æ–°ã—ã„é †
    items.sort(key=lambda x: (x["score"], x["published"]), reverse=True)
    items = items[:MAX_ARTICLES_TO_SEND]

    lines = ["ğŸ§‘â€ğŸ’¼ æ±‚è·è€…å‘ã‘ï¼šè»¢è·Ã—æ–½å·¥ç®¡ç†/è¨­å‚™ç®¡ç†ï¼ˆè©²å½“è¨˜äº‹ã®ã¿ï¼‰"]
    for it in items:
        summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
        lines.append(f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}")

    msg = "\n".join(lines)
    for chunk in split_for_line(msg):
        push_line(chunk)
        time.sleep(0.5)

if __name__ == "__main__":
    main()
