import os
import time
import re
import requests
import feedparser
from urllib.parse import quote_plus, unquote
from datetime import datetime, timezone, timedelta

# =========================
# ç›´è¿‘Næ—¥ã ã‘é€ã‚‹
# =========================
RECENT_DAYS = 3
REQUEST_TIMEOUT = 20
USER_AGENT = "Mozilla/5.0 (NewsBot/1.0)"

# =========================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­è¨ˆï¼ˆã‚ãªãŸå‘ã‘ï¼‰
# =========================
ROLE_TERMS = [
    "æ–½å·¥ç®¡ç†", "ç¾å ´ç›£ç£", "è¨­å‚™ç®¡ç†", "æ–½è¨­ç®¡ç†", "ãƒ“ãƒ«ãƒ¡ãƒ³", "è¨­å‚™ä¿å…¨",
    "ã‚µãƒ–ã‚³ãƒ³", "ã‚¼ãƒã‚³ãƒ³", "ç®¡å·¥äº‹", "é›»æ°—å·¥äº‹", "ç©ºèª¿", "è¡›ç”Ÿ"
]

CANDIDATE_TERMS = [
    "è»¢è·", "æ±‚äºº", "æ¡ç”¨", "ä¸­é€”", "äººæç´¹ä»‹", "ã‚­ãƒ£ãƒªã‚¢",
    "å¹´å", "çµ¦ä¸", "è³ƒä¸Šã’", "æ®‹æ¥­", "ä¼‘æ—¥", "é€±ä¼‘2æ—¥", "æœ‰çµ¦",
    "åƒãæ–¹æ”¹é©", "åŠ´åƒæ™‚é–“", "æœªçµŒé¨“", "çµŒé¨“è€…", "è³‡æ ¼"
]

INDUSTRY_TERMS = [
    "2024å¹´å•é¡Œ", "åƒãæ–¹æ”¹é©", "æ™‚é–“å¤–åŠ´åƒ", "æ³•æ”¹æ­£", "å»ºè¨­æ¥­æ³•", "ä¸‹è«‹",
    "äººæ‰‹ä¸è¶³", "é«˜é½¢åŒ–", "ä¾¡æ ¼è»¢å«", "è³‡æé«˜é¨°", "å…¬å…±å·¥äº‹", "å»ºè¨­æŠ•è³‡",
    "BIM", "CIM", "DX", "é éš”è‡¨å ´",
    "çœã‚¨ãƒ", "ZEB", "è„±ç‚­ç´ "
]

MAX_ARTICLES = 12  # 1å›ã«é€ã‚‹æœ€å¤§ä»¶æ•°

# =========================
# å–å¾—å…ƒï¼ˆæŒ‡å®š5ã‚µã‚¤ãƒˆï¼‰
# - RSSãŒå®‰å®šãªã‚‚ã®ã¯ç›´RSS
# - ãã‚Œä»¥å¤–ã¯ Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã§ site: ã—ã°ã‚Š
# =========================
def google_news_rss(query: str) -> str:
    return f"https://news.google.com/rss/search?q={quote_plus(query)}&hl=ja&gl=JP&ceid=JP:ja"

SITE_FEEDS = [
    ("æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒ†ãƒƒã‚¯", [
        google_news_rss("site:xtech.nikkei.com æ–½å·¥ç®¡ç†"),
        google_news_rss("site:xtech.nikkei.com è¨­å‚™ç®¡ç†"),
        google_news_rss("site:xtech.nikkei.com å»ºè¨­ äººæ‰‹ä¸è¶³"),
    ]),
    ("ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ãƒ»ã‚ªãƒ³ãƒ©ã‚¤ãƒ³", [
        google_news_rss("site:diamond.jp æ–½å·¥ç®¡ç†"),
        google_news_rss("site:diamond.jp è¨­å‚™ç®¡ç†"),
        google_news_rss("site:diamond.jp å»ºè¨­ æ¡ç”¨"),
    ]),
    ("NewsPicks", [
        google_news_rss("site:newspicks.com æ–½å·¥ç®¡ç†"),
        google_news_rss("site:newspicks.com è¨­å‚™ç®¡ç†"),
        google_news_rss("site:newspicks.com å»ºè¨­ æ¡ç”¨"),
    ]),
]

DIRECT_FEEDS = [
    ("ITmedia NEWS", ["https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"]),
    ("æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³", ["https://toyokeizai.net/list/feed/header"]),
]

SOURCES = DIRECT_FEEDS + SITE_FEEDS

# =========================
# LINEé€ä¿¡ï¼ˆSecretsï¼‰
# =========================
LINE_TOKEN = os.getenv("LINE_CHANNEL_TOKEN")
LINE_TO = os.getenv("LINE_TO_USER_ID")

if not (LINE_TOKEN and LINE_TO):
    raise RuntimeError("Secretsä¸è¶³: LINE_CHANNEL_TOKEN / LINE_TO_USER_ID ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def push_line(text: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LINE_TOKEN}"}
    body = {"to": LINE_TO, "messages": [{"type": "text", "text": text}]}
    r = requests.post(url, json=body, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def norm(s: str) -> str:
    s = (s or "").replace("\n", " ").replace("\r", " ").strip()
    while "  " in s:
        s = s.replace("  ", " ")
    return s

def count_hit(words, text: str) -> int:
    return sum(1 for w in words if w in text)

def get_published_dt(entry) -> datetime:
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        # æ—¥ä»˜ãŒå–ã‚Œãªã„ã‚‚ã®ã¯å¤ã„æ‰±ã„ï¼ˆç›´è¿‘ã ã‘ã«ã—ãŸã„ã®ã§ï¼‰
        return datetime(1970, 1, 1, tzinfo=timezone.utc)
    return datetime(*t[:6], tzinfo=timezone.utc)

def is_recent(published_dt: datetime) -> bool:
    now = datetime.now(timezone.utc)
    return published_dt >= (now - timedelta(days=RECENT_DAYS))

def resolve_final_url(url: str) -> str:
    """
    Google News RSSã® /rss/articles/... ã‚’å…ƒè¨˜äº‹URLã«è§£æ±ºã™ã‚‹ã€‚
    - 302ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã§å–ã‚Œã‚‹ãªã‚‰ãã‚Œã‚’æ¡ç”¨
    - å–ã‚Œãªã„å ´åˆã¯HTMLã‹ã‚‰å…ƒURLã‚’æŠ½å‡º
    - ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ç©ºæ–‡å­—
    """
    try:
        r = requests.get(
            url,
            allow_redirects=True,
            timeout=REQUEST_TIMEOUT,
            headers={"User-Agent": USER_AGENT},
        )
        if r.status_code >= 400:
            return ""

        final = (r.url or "").strip()

        # ã™ã§ã«å…ƒè¨˜äº‹ã«é£›ã¹ã¦ã„ã‚Œã°OK
        if final.startswith("http") and "news.google.com/rss/articles/" not in final:
            if "consent.google.com" in final:
                return ""
            return final

        # ã¾ã Googleã®RSSãƒªãƒ³ã‚¯ã®ã¾ã¾ãªã‚‰ã€HTMLã‹ã‚‰å…ƒè¨˜äº‹URLã‚’æŠœã
        html = r.text or ""

        # ã‚ˆãã‚ã‚‹å½¢ï¼šhttps://www.google.com/url?...&url=<encoded>&...
        m = re.search(r'https?://www\.google\.com/url\?[^"\']+', html)
        if m:
            u = m.group(0)
            m2 = re.search(r'url=([^&]+)', u)
            if m2:
                candidate = unquote(m2.group(1))
                if candidate.startswith("http") and "google.com" not in candidate and "news.google.com" not in candidate:
                    return candidate

        # åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šHTMLå†…ã« url=<encoded> ãŒåŸ‹ã¾ã£ã¦ã„ã‚‹
        for m3 in re.finditer(r'url=([^&"\']+)', html):
            candidate = unquote(m3.group(1))
            if candidate.startswith("http") and "google.com" not in candidate and "news.google.com" not in candidate:
                return candidate

        return ""
    except Exception:
        return ""

def split_for_line(message: str, limit: int = 4500):
    chunks, buf = [], ""
    for line in message.split("\n"):
        while len(line) > limit:
            head, line = line[:limit], line[limit:]
            if buf:
                chunks.append(buf); buf = ""
            chunks.append(head)
        if len(buf) + len(line) + 1 > limit:
            chunks.append(buf); buf = line
        else:
            buf = (buf + "\n" + line) if buf else line
    if buf:
        chunks.append(buf)
    return chunks

# =========================
# ãƒ¡ã‚¤ãƒ³ï¼ˆ2ã‚«ãƒ†ã‚´ãƒªã§é€ã‚‹ï¼‰
# =========================
def main():
    items = []
    seen = set()

    for source_name, feed_urls in SOURCES:
        for feed_url in feed_urls:
            feed = feedparser.parse(feed_url)

            for entry in feed.entries[:50]:
                title = norm(entry.get("title", ""))
                raw_link = entry.get("link", "")
                summary = norm(entry.get("summary", "") or entry.get("description", ""))

                if not raw_link:
                    continue

                published = get_published_dt(entry)
                if not is_recent(published):
                    continue

                # URLã‚’å…ƒè¨˜äº‹ã«è§£æ±ºï¼ˆæ­»ã‚“ã§ã‚‹/é£›ã¹ãªã„ç‡ã‚’ä¸‹ã’ã‚‹ï¼‰
                final_url = resolve_final_url(raw_link)
                if not final_url:
                    continue

                # Googleãƒ‹ãƒ¥ãƒ¼ã‚¹RSSã®ã¾ã¾æ®‹ã‚‹ãƒªãƒ³ã‚¯ã¯å£Šã‚Œã‚„ã™ã„ã®ã§é€ã‚‰ãªã„
                if "news.google.com/rss/articles/" in final_url:
                    continue

                if final_url in seen:
                    continue

                text = f"{title} {summary}"

                role_score = count_hit(ROLE_TERMS, text)
                cand_score = count_hit(CANDIDATE_TERMS, text)
                ind_score = count_hit(INDUSTRY_TERMS, text)

                # å»ºè¨­/è¨­å‚™ã®æ–‡è„ˆãŒè–„ã„ã‚‚ã®ã¯è½ã¨ã™ï¼ˆãƒã‚¤ã‚ºå¯¾ç­–ï¼‰
                if role_score == 0 and ind_score == 0:
                    continue

                items.append({
                    "source": source_name,
                    "title": title,
                    "link": final_url,
                    "summary": summary,
                    "published": published,
                    "role_score": role_score,
                    "cand_score": cand_score,
                    "ind_score": ind_score,
                })
                seen.add(final_url)

            time.sleep(0.2)

    if not items:
        push_line(f"âœ… ç›´è¿‘{RECENT_DAYS}æ—¥ã§è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå»ºè¨­/è¨­å‚™æ–‡è„ˆã®ãƒ’ãƒƒãƒˆãªã—ï¼‰")
        return

    # A) æ±‚è·è€…å‘ã‘ï¼šè»¢è·ãƒ»å¾…é‡ç³»ã‚’å¼·ãè©•ä¾¡
    cand_items = sorted(
        items,
        key=lambda x: (x["cand_score"]*100 + x["role_score"]*20 + x["ind_score"]*5, x["published"]),
        reverse=True
    )

    # B) æ¥­ç•Œç†è§£ï¼šåˆ¶åº¦ãƒ»å¸‚å ´ãƒ»DXç­‰ã‚’å¼·ãè©•ä¾¡
    ind_items = sorted(
        items,
        key=lambda x: (x["ind_score"]*100 + x["role_score"]*20 + x["cand_score"]*5, x["published"]),
        reverse=True
    )

    cand_top = [x for x in cand_items if x["cand_score"] > 0][:6]
    ind_top  = [x for x in ind_items if x["ind_score"] > 0][:6]

    # ä¿é™ºï¼šã©ã£ã¡ã‚‚ç©ºãªã‚‰ã€å½¹å‰²èªã§æ‹¾ãˆãŸã‚‚ã®ã‚’é€ã‚‹
    if not cand_top and not ind_top:
        fallback = sorted(items, key=lambda x: (x["role_score"], x["published"]), reverse=True)[:6]
        msg = f"ğŸ“° ç›´è¿‘{RECENT_DAYS}æ—¥ï¼šå»ºè¨­/è¨­å‚™ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆå‚è€ƒï¼‰\n"
        for it in fallback:
            summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
            msg += f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}\n"
        for chunk in split_for_line(msg):
            push_line(chunk); time.sleep(0.5)
        return

    lines = [f"ğŸ§‘â€ğŸ’¼ ç›´è¿‘{RECENT_DAYS}æ—¥ï¼šæ±‚è·è€…å‘ã‘ï¼ˆè»¢è·ãƒ»å¾…é‡ãƒ»åƒãæ–¹ï¼‰"]
    for it in cand_top:
        summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
        lines.append(f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}")

    lines.append(f"\nğŸ—ï¸ ç›´è¿‘{RECENT_DAYS}æ—¥ï¼šæ¥­ç•Œç†è§£ï¼ˆåˆ¶åº¦ãƒ»å¸‚å ´ãƒ»DXãƒ»çœã‚¨ãƒï¼‰")
    for it in ind_top:
        summ = it["summary"][:220] + ("â€¦" if len(it["summary"]) > 220 else "")
        lines.append(f"\n\n{it['title']}\nè¦æ—¨ï¼š{summ}\nURLï¼š{it['link']}")

    msg = "\n".join(lines)[:18000]  # æš´èµ°é˜²æ­¢
    for chunk in split_for_line(msg):
        push_line(chunk)
        time.sleep(0.5)

if __name__ == "__main__":
    main()
