import os
import time
import requests
import feedparser
import trafilatura

# =========================
# 1) ã‚ãªãŸå‘ã‘ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ–½å·¥ç®¡ç†ãƒ»è¨­å‚™ç®¡ç†ãƒ»å»ºè¨­Ã—äººæï¼‰
# =========================
KEYWORDS = [
    # æ–½å·¥ç®¡ç†ãƒ»è¨­å‚™ç®¡ç†
    "æ–½å·¥ç®¡ç†", "è¨­å‚™ç®¡ç†", "ãƒ“ãƒ«ãƒ¡ãƒ³", "æ–½è¨­ç®¡ç†", "FM", "ç¾å ´ç›£ç£",
    "é›»æ°—å·¥äº‹æ–½å·¥ç®¡ç†", "ç®¡å·¥äº‹æ–½å·¥ç®¡ç†", "å»ºç¯‰æ–½å·¥ç®¡ç†", "åœŸæœ¨æ–½å·¥ç®¡ç†",
    "é›»æ°—ä¸»ä»»æŠ€è¡“è€…", "ç›£ç†æŠ€è¡“è€…", "ä¸»ä»»æŠ€è¡“è€…", "æŠ€è¡“å£«",
    "1ç´šæ–½å·¥ç®¡ç†æŠ€å£«", "2ç´šæ–½å·¥ç®¡ç†æŠ€å£«", "å»ºç¯‰å£«",

    # äººæãƒ»è»¢è·å¸‚å ´
    "äººæ‰‹ä¸è¶³", "æ¡ç”¨", "è»¢è·", "è³ƒä¸Šã’", "åˆä»»çµ¦", "å¹´å", "æ®‹æ¥­", "2024å¹´å•é¡Œ",
    "åƒãæ–¹æ”¹é©", "æ™‚é–“å¤–åŠ´åƒ", "é€±ä¼‘2æ—¥", "36å”å®š",
    "æ´¾é£", "å¤–æ³¨", "å”åŠ›ä¼šç¤¾", "ä¸‹è«‹æ³•",

    # å»ºè¨­DX
    "BIM", "CIM", "i-Construction", "DX", "ç¾å ´DX", "é éš”è‡¨å ´", "é›»å­é»’æ¿",
    "ç©ç®—", "åŸä¾¡ç®¡ç†", "å·¥ç¨‹ç®¡ç†",

    # è¨­å‚™ãƒ»çœã‚¨ãƒ
    "çœã‚¨ãƒ", "ZEB", "ZEH", "è„±ç‚­ç´ ", "ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«", "BEMS",
    "ç©ºèª¿", "HVAC", "å†·å‡æ©Ÿ", "ãƒœã‚¤ãƒ©ãƒ¼", "æ¶ˆé˜²è¨­å‚™", "é˜²ç½", "ç‚¹æ¤œ", "æ³•æ”¹æ­£",

    # ç™ºæ³¨å´ãƒ»ç”¨é€”
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼", "å·¥å ´", "ãƒ—ãƒ©ãƒ³ãƒˆ", "ç‰©æµå€‰åº«", "ç—…é™¢", "å•†æ¥­æ–½è¨­",
    "ä¸å‹•ç”£ç®¡ç†", "PM", "BM", "AM", "REIT"
]

# =========================
# 2) èª¿æ•´ï¼ˆæœ€åˆã¯ã“ã®ã¾ã¾ã§OKï¼‰
# =========================
MAX_ARTICLES_TO_SEND = 10      # 1å›ã«é€ã‚‹æœ€å¤§è¨˜äº‹æ•°
REQUEST_TIMEOUT = 20

# ã¾ãšã¯å®‰å®šã—ã¦RSSãŒå–ã‚Œã‚‹2ã‚µã‚¤ãƒˆï¼ˆã“ã®å¾Œ5ã‚µã‚¤ãƒˆã«å¢—ã‚„ã›ã¾ã™ï¼‰
RSS_SOURCES = [
    ("ITmedia NEWS", "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"),
    ("æ±æ´‹çµŒæ¸ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³", "https://toyokeizai.net/list/feed/header"),
]

# =========================
# 3) Secretsï¼ˆGitHubã«ç™»éŒ²ã—ãŸã‚‚ã®ï¼‰
# =========================
LINE_TOKEN = os.getenv("LINE_CHANNEL_TOKEN")
LINE_TO = os.getenv("LINE_TO_USER_ID")

if not (LINE_TOKEN and LINE_TO):
    raise RuntimeError("Secretsä¸è¶³: LINE_CHANNEL_TOKEN / LINE_TO_USER_ID ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def keyword_hit(text: str) -> bool:
    t = text or ""
    return any(k in t for k in KEYWORDS)

def normalize_text(s: str) -> str:
    s = (s or "").replace("\n", " ").replace("\r", " ").strip()
    while "  " in s:
        s = s.replace("  ", " ")
    return s

def try_fetch_body_preview(url: str, max_chars: int = 240) -> str:
    """
    å¯èƒ½ãªã‚‰æœ¬æ–‡ã‚’å°‘ã—ã ã‘å–ã£ã¦â€œæ¦‚è¦è£œå¼·â€ã«ä½¿ã†ï¼ˆç„¡æ–™ãƒ»è¦ç´„ãªã—ï¼‰ã€‚
    å–ã‚Œãªã„å ´åˆã¯ç©ºæ–‡å­—ã€‚
    """
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return ""
        extracted = trafilatura.extract(downloaded)
        if not extracted:
            return ""
        extracted = normalize_text(extracted)
        return extracted[:max_chars]
    except Exception:
        return ""

def push_line(message: str) -> None:
    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_TOKEN}",
    }
    body = {"to": LINE_TO, "messages": [{"type": "text", "text": message}]}
    r = requests.post(url, json=body, headers=headers, timeout=REQUEST_TIMEOUT)
    r.raise_for_status()

def main():
    picked = []

    # RSSå–å¾— â†’ ã‚¿ã‚¤ãƒˆãƒ«ï¼‹æ¦‚è¦ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
    for source_name, rss_url in RSS_SOURCES:
        feed = feedparser.parse(rss_url)
        for entry in feed.entries[:40]:
            title = entry.get("title", "")
            link = entry.get("link", "")
            snippet = entry.get("summary", "") or entry.get("description", "") or ""
            if not link:
                continue

            hay = f"{title} {snippet}"
            if keyword_hit(hay):
                picked.append((source_name, title, link, snippet))

    if not picked:
        push_line("âœ… ä»Šæ—¥ã®è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´ãªã—ï¼‰")
        return

    picked = picked[:MAX_ARTICLES_TO_SEND]

    lines = ["ğŸ“° ä»Šæ—¥ã®è©²å½“ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆç„¡æ–™ç‰ˆï¼šã‚¿ã‚¤ãƒˆãƒ«ï¼‹æ¦‚è¦ï¼‹URLï¼‰"]
    for source_name, title, link, snippet in picked:
        snippet = normalize_text(snippet)
        body_preview = try_fetch_body_preview(link)
        # â€œæ¦‚è¦â€ãŒçŸ­ã„/ç©ºãªã‚‰æœ¬æ–‡ã®å†’é ­ã‚’è£œè¶³ã¨ã—ã¦ä½¿ã†
        preview = snippet if snippet else body_preview
        if body_preview and snippet and (len(snippet) < 80):
            preview = f"{snippet} / æœ¬æ–‡å†’é ­: {body_preview}"
        elif body_preview and not snippet:
            preview = f"æœ¬æ–‡å†’é ­: {body_preview}"

        if preview:
            preview = preview[:260] + ("â€¦" if len(preview) > 260 else "")

        lines.append(f"\n\n{title}\næ¦‚è¦ï¼š{preview}\nURLï¼š{link}")
        time.sleep(0.5)

    push_line("\n".join(lines))

if __name__ == "__main__":
    main()
